{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFaPUz7NMvD1"
      },
      "source": [
        "# **MINST Code**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "_gqrlPqigaeQ"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist                                                # Loading the MNIST dataset in Keras\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "from keras import models                                                        # Setting up the network architecture\n",
        "from keras import layers\n",
        "network = models.Sequential()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "8KXtTn6kkMT3"
      },
      "outputs": [],
      "source": [
        "network.add(layers.Dense(512, activation='relu', input_shape=(28 *\n",
        "28,)))                                                                          # 512 is 2/3 of 784 which is 28*28.\n",
        "                                                                                # ‘relu’ stands for rectified linear activation function.\n",
        "network.add(layers.Dense(10, activation='softmax'))                             # ‘softmax’ yields the probability of all output nodes between 0 and 1. The\n",
        "                                                                                # added total value is 1.\n",
        "network.compile (optimizer='rmsprop',\n",
        "loss='categorical_crossentropy', metrics=['accuracy'])                          # Loss is the diff between actual and target,\n",
        "                                                                                # optimizer is the mechanism to apply the ‘loss’ value to the next epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "a8UjU4l-kLP8"
      },
      "outputs": [],
      "source": [
        "train_images = train_images.reshape((60000, 28 * 28))                           # Normalization so all values fall between (0,1)\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))                             # Previously, our training images, for instance, were stored in an array of\n",
        "                                                                                # shape (60000, 28, 28) of type uint8 with values in the [0, 255] interval.\n",
        "test_images = test_images.astype('float32') / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "bppky8nYwJoa"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical                                          # We also need to categorically encode the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UcrMWLhzwTuj",
        "outputId": "529e3ad0-5c15-47bf-f7e1-4adbe0c518df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 23ms/step - accuracy: 0.8734 - loss: 0.4401\n",
            "Epoch 2/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - accuracy: 0.9657 - loss: 0.1165\n",
            "Epoch 3/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.9787 - loss: 0.0713\n",
            "Epoch 4/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 12ms/step - accuracy: 0.9844 - loss: 0.0512\n",
            "Epoch 5/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9884 - loss: 0.0382\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9754 - loss: 0.0853\n"
          ]
        }
      ],
      "source": [
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
        "test_loss, test_acc = network.evaluate(test_images, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "o-eq8wlawe1M",
        "outputId": "e1471a79-1eb2-4e97-958c-9630bbb2ee8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_acc: 0.9789000153541565\n"
          ]
        }
      ],
      "source": [
        "print('test_acc:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUeB5icSMzN_"
      },
      "source": [
        "# **IMDB Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "DXyv-Z9EM5Cj"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import imdb\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "FTQs3B8PW6NZ"
      },
      "outputs": [],
      "source": [
        "# Load dataset and we are limiting the number of words to 10000 to make the array of words more managable\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "                                                                                # train_data and test_data contain sequences of word indices, labels are 0 or 1 (negative/positive)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "M_D9-OTnNYeP",
        "outputId": "54a525e6-a2d7-46d5-915b-dd8188897531"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 2,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 2,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 2,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 5244,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 2,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 5952,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 2,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 2,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 7486,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 5535,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 5345,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ],
      "source": [
        "train_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KBP0yTD8XVio",
        "outputId": "2376ab1e-f91e-45e0-d52c-928a1337b125"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ],
      "source": [
        "train_labels[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "NEfXFaFlXuiG"
      },
      "outputs": [],
      "source": [
        "word_index = imdb.get_word_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "eyr_CToDYL0h"
      },
      "outputs": [],
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in\n",
        "word_index.items()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "-soo9LMKQ-bm"
      },
      "outputs": [],
      "source": [
        "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in\n",
        "train_data[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "P5q8G1X4RCbi"
      },
      "outputs": [],
      "source": [
        "# Function to vectorize sequences into 10,000-dimensional one-hot encoded vectors\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))                             # Create an array of zeros with shape (num_sequences, 10000)\n",
        "    for i, sequence in enumerate(sequences):                                    # Loop over each sequence in sequences\n",
        "        results[i, sequence] = 1                                                # Set the corresponding indices to 1 for each sequence\n",
        "    return results                                                              # Return the one-hot encoded matrix\n",
        "                                                                                # mean, for instance, turning the sequence [3, 5] into a 10,000-dimensional\n",
        "                                                                                # vector that would be all 0s except for indices 3 and 5, which would be 1s.\n",
        "                                                                                # Then you could use as the first layer in your network a Dense layer, capable\n",
        "                                                                                # of handling floating-point vector data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "DIb5PjgRSDJg"
      },
      "outputs": [],
      "source": [
        "# Vectorize the training and test data\n",
        "x_train = vectorize_sequences(train_data)                                       # Convert the train_data into one-hot encoded vectors\n",
        "x_test = vectorize_sequences(test_data)                                         # Convert the test_data into one-hot encoded vectors\n",
        "\n",
        "# Convert labels to float32 data type\n",
        "y_train = np.asarray(train_labels).astype('float32')                            # Convert the train_labels to float32\n",
        "y_test = np.asarray(test_labels).astype('float32')                              # Convert the test_labels to float32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "S-sdR8ZUScBo"
      },
      "outputs": [],
      "source": [
        "# Re-build and re-train the model for 4 epochs based on validation performance\n",
        "model = models.Sequential()                                                     # Initialize a new sequential model\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))            # Add first Dense layer with 16 units, relu activation, and input shape of 10,000\n",
        "model.add(layers.Dense(16, activation='relu'))                                  # Add second Dense layer with 16 units and relu activation\n",
        "model.add(layers.Dense(1, activation='sigmoid'))                                # Add output layer with 1 unit and sigmoid activation for binary classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "GITVuRT5U-oB"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy',\n",
        "metrics=['accuracy'])\n",
        "                                                                                # it’s best to use the binary_crossentropy loss. It isn’t the only viable choice:\n",
        "                                                                                # you could use, for instance, mean_squared_error. But crossentropy is\n",
        "                                                                                # usually the best choice when you’re dealing with models that output\n",
        "                                                                                # probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "o5_GhwCrTGoG"
      },
      "outputs": [],
      "source": [
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "GC0lIqVjjjeP",
        "outputId": "94bdf6b5-a5eb-441b-e6d0-571920dc02ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - acc: 0.5807 - loss: 0.6426 - val_acc: 0.8621 - val_loss: 0.4670\n",
            "Epoch 2/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - acc: 0.8787 - loss: 0.4118 - val_acc: 0.8374 - val_loss: 0.3892\n",
            "Epoch 3/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - acc: 0.9177 - loss: 0.2884 - val_acc: 0.8856 - val_loss: 0.3061\n",
            "Epoch 4/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - acc: 0.9362 - loss: 0.2176 - val_acc: 0.8882 - val_loss: 0.2849\n",
            "Epoch 5/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - acc: 0.9513 - loss: 0.1705 - val_acc: 0.8859 - val_loss: 0.2855\n",
            "Epoch 6/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - acc: 0.9594 - loss: 0.1443 - val_acc: 0.8858 - val_loss: 0.2904\n",
            "Epoch 7/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - acc: 0.9666 - loss: 0.1203 - val_acc: 0.8797 - val_loss: 0.3029\n",
            "Epoch 8/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - acc: 0.9732 - loss: 0.1043 - val_acc: 0.8776 - val_loss: 0.3121\n",
            "Epoch 9/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - acc: 0.9805 - loss: 0.0843 - val_acc: 0.8640 - val_loss: 0.3663\n",
            "Epoch 10/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - acc: 0.9818 - loss: 0.0758 - val_acc: 0.8733 - val_loss: 0.3445\n",
            "Epoch 11/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - acc: 0.9883 - loss: 0.0602 - val_acc: 0.8795 - val_loss: 0.3667\n",
            "Epoch 12/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - acc: 0.9929 - loss: 0.0482 - val_acc: 0.8694 - val_loss: 0.3883\n",
            "Epoch 13/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - acc: 0.9938 - loss: 0.0421 - val_acc: 0.8735 - val_loss: 0.3953\n",
            "Epoch 14/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - acc: 0.9930 - loss: 0.0391 - val_acc: 0.8785 - val_loss: 0.4196\n",
            "Epoch 15/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - acc: 0.9965 - loss: 0.0300 - val_acc: 0.8588 - val_loss: 0.4816\n",
            "Epoch 16/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - acc: 0.9975 - loss: 0.0250 - val_acc: 0.8764 - val_loss: 0.4738\n",
            "Epoch 17/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - acc: 0.9981 - loss: 0.0191 - val_acc: 0.8739 - val_loss: 0.4685\n",
            "Epoch 18/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - acc: 0.9986 - loss: 0.0153 - val_acc: 0.8670 - val_loss: 0.4939\n",
            "Epoch 19/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - acc: 0.9988 - loss: 0.0133 - val_acc: 0.8735 - val_loss: 0.5069\n",
            "Epoch 20/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - acc: 0.9977 - loss: 0.0136 - val_acc: 0.8732 - val_loss: 0.5270\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "# Train the model for 20 epochs with mini-batches of 512 samples and validation data\n",
        "history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))\n",
        "# Train the model on partial_x_train and partial_y_train, validate on x_val and y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "rkagD1Y9j3lL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "bd9ee812-9483-45b1-a4cb-73e5e4a1b150"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['acc', 'loss', 'val_acc', 'val_loss'])"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ],
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "QlLkBgeqk4ak",
        "outputId": "ed9330d7-6fa2-4277-b2a0-31a9f6cb1d7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7375 - loss: 0.5488\n",
            "Epoch 2/4\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9042 - loss: 0.2781\n",
            "Epoch 3/4\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9292 - loss: 0.2063\n",
            "Epoch 4/4\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.9389 - loss: 0.1781\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d8a3d1958a0>"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ],
      "source": [
        "# Re-build and re-train the model for 4 epochs based on validation performance\n",
        "model = models.Sequential()  # Initialize a new sequential model\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))  # Add first Dense layer with 16 units, relu activation, and input shape of 10,000\n",
        "model.add(layers.Dense(16, activation='relu'))  # Add second Dense layer with 16 units and relu activation\n",
        "model.add(layers.Dense(1, activation='sigmoid'))  # Add output layer with 1 unit and sigmoid activation for binary classification\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])  # Compile the new model\n",
        "model.fit(x_train, y_train, epochs=4, batch_size=512)  # Train the new model on the entire x_train and y_train for 4 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "H3XM-zmklCE6",
        "outputId": "612629df-d924-43a6-a8ca-1582427e344d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8820 - loss: 0.2893\n"
          ]
        }
      ],
      "source": [
        "results = model.evaluate(x_test, y_test)  # Evaluate the model on x_test and y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6iaqWUAM5yQ"
      },
      "source": [
        "# **Boston_housing Code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-x1_4KSUI5B"
      },
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "zuDviEcnSMAB"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import boston_housing                                       # This cell loads the boston_housing dataset from Keras (Keras is a popular, open source Python library that is used to build and train deep learning models)\n",
        "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "ryRFO3COSNlp"
      },
      "outputs": [],
      "source": [
        "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()  # This cell calls the load_data function and assigns the result to train_data and other associated variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xfFN06O-SOEy",
        "outputId": "4be1d507-2b87-4a0e-a350-7b5c16056a74"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(404, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ],
      "source": [
        "train_data.shape                                                                # The output will show the shape of train_data\n",
        "                                                                                # We have 404 training samples with 13 numerical features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "iMnPjMBCSbt8",
        "outputId": "cd65a832-8fbd-49d5-f9ce-b0090b715b07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(102, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ],
      "source": [
        "test_data.shape                                                                 # The output will show the shape of test_data\n",
        "                                                                                # We have 102 test samples with 13 numerical features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SgqMRngcS3Ll",
        "outputId": "d21a3755-119f-4036-89fd-2dbc8cf53e8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4, 12.1,\n",
              "       17.9, 23.1, 19.9, 15.7,  8.8, 50. , 22.5, 24.1, 27.5, 10.9, 30.8,\n",
              "       32.9, 24. , 18.5, 13.3, 22.9, 34.7, 16.6, 17.5, 22.3, 16.1, 14.9,\n",
              "       23.1, 34.9, 25. , 13.9, 13.1, 20.4, 20. , 15.2, 24.7, 22.2, 16.7,\n",
              "       12.7, 15.6, 18.4, 21. , 30.1, 15.1, 18.7,  9.6, 31.5, 24.8, 19.1,\n",
              "       22. , 14.5, 11. , 32. , 29.4, 20.3, 24.4, 14.6, 19.5, 14.1, 14.3,\n",
              "       15.6, 10.5,  6.3, 19.3, 19.3, 13.4, 36.4, 17.8, 13.5, 16.5,  8.3,\n",
              "       14.3, 16. , 13.4, 28.6, 43.5, 20.2, 22. , 23. , 20.7, 12.5, 48.5,\n",
              "       14.6, 13.4, 23.7, 50. , 21.7, 39.8, 38.7, 22.2, 34.9, 22.5, 31.1,\n",
              "       28.7, 46. , 41.7, 21. , 26.6, 15. , 24.4, 13.3, 21.2, 11.7, 21.7,\n",
              "       19.4, 50. , 22.8, 19.7, 24.7, 36.2, 14.2, 18.9, 18.3, 20.6, 24.6,\n",
              "       18.2,  8.7, 44. , 10.4, 13.2, 21.2, 37. , 30.7, 22.9, 20. , 19.3,\n",
              "       31.7, 32. , 23.1, 18.8, 10.9, 50. , 19.6,  5. , 14.4, 19.8, 13.8,\n",
              "       19.6, 23.9, 24.5, 25. , 19.9, 17.2, 24.6, 13.5, 26.6, 21.4, 11.9,\n",
              "       22.6, 19.6,  8.5, 23.7, 23.1, 22.4, 20.5, 23.6, 18.4, 35.2, 23.1,\n",
              "       27.9, 20.6, 23.7, 28. , 13.6, 27.1, 23.6, 20.6, 18.2, 21.7, 17.1,\n",
              "        8.4, 25.3, 13.8, 22.2, 18.4, 20.7, 31.6, 30.5, 20.3,  8.8, 19.2,\n",
              "       19.4, 23.1, 23. , 14.8, 48.8, 22.6, 33.4, 21.1, 13.6, 32.2, 13.1,\n",
              "       23.4, 18.9, 23.9, 11.8, 23.3, 22.8, 19.6, 16.7, 13.4, 22.2, 20.4,\n",
              "       21.8, 26.4, 14.9, 24.1, 23.8, 12.3, 29.1, 21. , 19.5, 23.3, 23.8,\n",
              "       17.8, 11.5, 21.7, 19.9, 25. , 33.4, 28.5, 21.4, 24.3, 27.5, 33.1,\n",
              "       16.2, 23.3, 48.3, 22.9, 22.8, 13.1, 12.7, 22.6, 15. , 15.3, 10.5,\n",
              "       24. , 18.5, 21.7, 19.5, 33.2, 23.2,  5. , 19.1, 12.7, 22.3, 10.2,\n",
              "       13.9, 16.3, 17. , 20.1, 29.9, 17.2, 37.3, 45.4, 17.8, 23.2, 29. ,\n",
              "       22. , 18. , 17.4, 34.6, 20.1, 25. , 15.6, 24.8, 28.2, 21.2, 21.4,\n",
              "       23.8, 31. , 26.2, 17.4, 37.9, 17.5, 20. ,  8.3, 23.9,  8.4, 13.8,\n",
              "        7.2, 11.7, 17.1, 21.6, 50. , 16.1, 20.4, 20.6, 21.4, 20.6, 36.5,\n",
              "        8.5, 24.8, 10.8, 21.9, 17.3, 18.9, 36.2, 14.9, 18.2, 33.3, 21.8,\n",
              "       19.7, 31.6, 24.8, 19.4, 22.8,  7.5, 44.8, 16.8, 18.7, 50. , 50. ,\n",
              "       19.5, 20.1, 50. , 17.2, 20.8, 19.3, 41.3, 20.4, 20.5, 13.8, 16.5,\n",
              "       23.9, 20.6, 31.5, 23.3, 16.8, 14. , 33.8, 36.1, 12.8, 18.3, 18.7,\n",
              "       19.1, 29. , 30.1, 50. , 50. , 22. , 11.9, 37.6, 50. , 22.7, 20.8,\n",
              "       23.5, 27.9, 50. , 19.3, 23.9, 22.6, 15.2, 21.7, 19.2, 43.8, 20.3,\n",
              "       33.2, 19.9, 22.5, 32.7, 22. , 17.1, 19. , 15. , 16.1, 25.1, 23.7,\n",
              "       28.7, 37.2, 22.6, 16.4, 25. , 29.8, 22.1, 17.4, 18.1, 30.3, 17.5,\n",
              "       24.7, 12.6, 26.5, 28.7, 13.3, 10.4, 24.4, 23. , 20. , 17.8,  7. ,\n",
              "       11.8, 24.4, 13.8, 19.4, 25.2, 19.4, 19.4, 29.1])"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ],
      "source": [
        "train_targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJoYds7LUG6H"
      },
      "source": [
        "Normalizing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOXgdkrRUWrx"
      },
      "source": [
        "The normalization is done with feature-wise normalization: for each feature in the input data (a column in the input data matrix), you subtract the mean of the feature and divide by the standard deviation, so that the feature is centered around 0 and has a unit standard deviation. This is easily done in Numpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "jcCx1yPHUM29"
      },
      "outputs": [],
      "source": [
        "mean = train_data.mean (axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "Oum5lBSPUw66"
      },
      "outputs": [],
      "source": [
        "train_data -= mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "zvIoTPluU-rN"
      },
      "outputs": [],
      "source": [
        "std = train_data.std (axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "H2qkeXT5VG5J"
      },
      "outputs": [],
      "source": [
        "train_data /= std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki4Ljk9ne0xZ"
      },
      "source": [
        "Building the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HIDvlDfe667"
      },
      "source": [
        "Because so few sample are available, you'll use a very small network with two hidden layers, each with 64 units. In general, the less training data you have, the worse overfitting will be, and using a small network is one way to mitigate overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "pG-RpnlXhC4A"
      },
      "outputs": [],
      "source": [
        "from keras import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "Wcpcw1dVhYSR"
      },
      "outputs": [],
      "source": [
        "from keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "EAPjaVM2r70B"
      },
      "outputs": [],
      "source": [
        "def build_model():                                                              # having issues with \"de build_model():\" either a syntax error and/or not defined\n",
        "  model = models.Sequential()                                                   # this issue was solved after returning the model (I was running functions while entering, so the function was incomplete)\n",
        "  model.add(layers.Dense(64, activation=\"relu\",\n",
        "                         input_shape=(train_data.shape[1],)))                   # had some errors; switched '' to \"\"\n",
        "  model.add (layers.Dense(64, activation = \"relu\"))\n",
        "  model.add (layers.Dense(1))\n",
        "  model.compile (optimizer = \"rmsprop\", loss = \"mse\",                           # complie the network with the mse loss function - mean squared error, the square\n",
        "                 metrics = [\"mae\"])                                             # of the difference between the predictions and the targets. This is a widely used loss function for regressio problems.\n",
        "  return model                                                                  # You're also monitoring a new metric during training: mean absolute error (mae). It's the absolute value of the difference between the predictions and the targets.\n",
        "                                                                                # For instance, an MAE of 0.5 of this problem would mean your predictions are off by $500 on average."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BgbiO074a1N"
      },
      "source": [
        "Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UOFolgZGsQ2e",
        "outputId": "14d45466-3c31-4253-9941-1b70159ab10f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing fold # 0\n",
            "processing fold # 1\n",
            "processing fold # 2\n",
            "processing fold # 3\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "k=4\n",
        "num_val_samples = len(train_data) // k\n",
        "num_epochs = 100\n",
        "all_scores = [ ]\n",
        "for i in range (k):\n",
        "  print (\"processing fold #\", i)                                                # had some errors; switched '' to \"\",\n",
        "  val_data = train_data [i * num_val_samples: (i+1) * num_val_samples]          # prepares the validation data: data from partition #k\n",
        "  val_targets = train_targets [i * num_val_samples: (i+1) * num_val_samples]\n",
        "  partial_train_data = np.concatenate(                                          # prepares the training data: data from all other partitions\n",
        "      [train_data [:i * num_val_samples],\n",
        "       train_data [(i+1)*num_val_samples:]], axis = 0)\n",
        "  partial_train_targets = np.concatenate(\n",
        "      [train_targets [:i*num_val_samples],\n",
        "       train_targets [(i+1)*num_val_samples:]], axis = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sveiAH9-6iaw"
      },
      "source": [
        "Running this with num_epochs = 100 yields the following results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PMbJgu3WeEa_",
        "outputId": "e89ea869-3787-4c5d-fbf1-b0e418d23ab5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ],
      "source": [
        "all_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jz0FBxg69yp"
      },
      "source": [
        "Saving the Validation logs at each fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "pv-FlfMV7An4"
      },
      "outputs": [],
      "source": [
        "num_epochs = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1V5SAXOP7C5p",
        "outputId": "9b29e851-037c-4c04-8171-f8cdaa84d019"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing fold # 0\n",
            "processing fold # 1\n",
            "processing fold # 2\n",
            "processing fold # 3\n"
          ]
        }
      ],
      "source": [
        "all_mae_histories = []\n",
        "for i in range (k):\n",
        "  print (\"processing fold #\", i)\n",
        "  val_data = train_data [i*num_val_samples:(i+1)*num_val_samples]               # prepares the validationdata: data from partition #k\n",
        "  val_targets = train_targets [i*num_val_samples: (i+1)*num_val_samples]\n",
        "  partial_train_data = np.concatenate(                                          # prepares the training data: data from all other partitions\n",
        "      [train_data [:i*num_val_samples],\n",
        "       train_data [(i+1)*num_val_samples:]], axis = 0)\n",
        "  partial_train_targets = np.concatenate(\n",
        "      [train_targets [:i*num_val_samples],\n",
        "       train_targets [(i+1)*num_val_samples:]], axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "0bGW8y8M7uEt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "71d9dc38-725d-4469-a580-4a59672aeae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 297.4845 - mae: 14.8006 - val_loss: 74.4277 - val_mae: 6.0654\n",
            "Epoch 2/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 35.6500 - mae: 4.0552 - val_loss: 44.8542 - val_mae: 4.3246\n",
            "Epoch 3/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 22.3736 - mae: 3.2495 - val_loss: 30.0274 - val_mae: 3.5527\n",
            "Epoch 4/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 16.4400 - mae: 2.7693 - val_loss: 27.3627 - val_mae: 3.1481\n",
            "Epoch 5/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 20.5923 - mae: 2.7897 - val_loss: 22.1948 - val_mae: 2.9786\n",
            "Epoch 6/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 13.4198 - mae: 2.3695 - val_loss: 21.1174 - val_mae: 2.9629\n",
            "Epoch 7/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 11.9211 - mae: 2.2119 - val_loss: 20.3410 - val_mae: 2.8309\n",
            "Epoch 8/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 15.4125 - mae: 2.3622 - val_loss: 16.9282 - val_mae: 2.5582\n",
            "Epoch 9/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.6782 - mae: 1.8650 - val_loss: 18.0552 - val_mae: 2.6344\n",
            "Epoch 10/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.8548 - mae: 2.0095 - val_loss: 16.4189 - val_mae: 2.5581\n",
            "Epoch 11/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 10.1627 - mae: 2.1226 - val_loss: 20.0751 - val_mae: 2.8163\n",
            "Epoch 12/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.1913 - mae: 1.9750 - val_loss: 15.2997 - val_mae: 2.4627\n",
            "Epoch 13/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 12.7100 - mae: 2.2673 - val_loss: 16.2318 - val_mae: 2.5789\n",
            "Epoch 14/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 10.5472 - mae: 2.0379 - val_loss: 13.2969 - val_mae: 2.4419\n",
            "Epoch 15/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 11.2915 - mae: 2.2759 - val_loss: 17.1917 - val_mae: 2.6340\n",
            "Epoch 16/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.9235 - mae: 1.7525 - val_loss: 14.2192 - val_mae: 2.5351\n",
            "Epoch 17/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.8514 - mae: 1.9144 - val_loss: 15.4781 - val_mae: 2.6586\n",
            "Epoch 18/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.9868 - mae: 1.9147 - val_loss: 13.8855 - val_mae: 2.7065\n",
            "Epoch 19/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 9.0876 - mae: 2.0988 - val_loss: 11.6596 - val_mae: 2.3461\n",
            "Epoch 20/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 8.8804 - mae: 1.9132 - val_loss: 12.7992 - val_mae: 2.4175\n",
            "Epoch 21/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6.9146 - mae: 1.8054 - val_loss: 14.2882 - val_mae: 2.5573\n",
            "Epoch 22/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.6761 - mae: 1.8318 - val_loss: 12.7522 - val_mae: 2.5817\n",
            "Epoch 23/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.4730 - mae: 1.9181 - val_loss: 12.7696 - val_mae: 2.4559\n",
            "Epoch 24/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.7141 - mae: 2.0441 - val_loss: 13.2264 - val_mae: 2.5515\n",
            "Epoch 25/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.3727 - mae: 1.8201 - val_loss: 14.0092 - val_mae: 2.7041\n",
            "Epoch 26/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.9268 - mae: 1.7783 - val_loss: 12.1937 - val_mae: 2.5010\n",
            "Epoch 27/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.8925 - mae: 1.9927 - val_loss: 12.6062 - val_mae: 2.5270\n",
            "Epoch 28/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.4518 - mae: 1.8576 - val_loss: 14.9242 - val_mae: 2.6770\n",
            "Epoch 29/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.6186 - mae: 1.7616 - val_loss: 10.8649 - val_mae: 2.3597\n",
            "Epoch 30/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.3713 - mae: 1.7162 - val_loss: 13.0384 - val_mae: 2.6784\n",
            "Epoch 31/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.5106 - mae: 1.6386 - val_loss: 12.7197 - val_mae: 2.6045\n",
            "Epoch 32/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.7671 - mae: 1.8568 - val_loss: 11.6942 - val_mae: 2.4232\n",
            "Epoch 33/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.8750 - mae: 1.8671 - val_loss: 11.8034 - val_mae: 2.4946\n",
            "Epoch 34/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6.8992 - mae: 1.7685 - val_loss: 11.2544 - val_mae: 2.4293\n",
            "Epoch 35/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7.5009 - mae: 1.7038 - val_loss: 11.8520 - val_mae: 2.5633\n",
            "Epoch 36/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.5608 - mae: 1.5484 - val_loss: 10.9749 - val_mae: 2.4350\n",
            "Epoch 37/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.9637 - mae: 1.7792 - val_loss: 12.6662 - val_mae: 2.5103\n",
            "Epoch 38/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.5520 - mae: 1.7231 - val_loss: 12.3803 - val_mae: 2.5003\n",
            "Epoch 39/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.4719 - mae: 1.6215 - val_loss: 11.1677 - val_mae: 2.4275\n",
            "Epoch 40/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.8618 - mae: 1.6086 - val_loss: 10.8102 - val_mae: 2.4110\n",
            "Epoch 41/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.6179 - mae: 1.4667 - val_loss: 12.0814 - val_mae: 2.4841\n",
            "Epoch 42/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.4214 - mae: 1.6554 - val_loss: 10.7063 - val_mae: 2.3447\n",
            "Epoch 43/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.7067 - mae: 1.7132 - val_loss: 13.1848 - val_mae: 2.7247\n",
            "Epoch 44/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.8228 - mae: 1.5639 - val_loss: 13.2741 - val_mae: 2.6120\n",
            "Epoch 45/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.9158 - mae: 1.5939 - val_loss: 10.4834 - val_mae: 2.3100\n",
            "Epoch 46/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.8128 - mae: 1.5181 - val_loss: 10.9378 - val_mae: 2.4160\n",
            "Epoch 47/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.4995 - mae: 1.4720 - val_loss: 14.0511 - val_mae: 2.7045\n",
            "Epoch 48/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.1064 - mae: 1.5678 - val_loss: 12.5651 - val_mae: 2.5810\n",
            "Epoch 49/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.6737 - mae: 1.6634 - val_loss: 12.0744 - val_mae: 2.6107\n",
            "Epoch 50/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.3506 - mae: 1.4528 - val_loss: 14.6206 - val_mae: 3.0062\n",
            "Epoch 51/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.7948 - mae: 1.6060 - val_loss: 11.1429 - val_mae: 2.3868\n",
            "Epoch 52/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 5.8008 - mae: 1.6878 - val_loss: 11.7641 - val_mae: 2.4947\n",
            "Epoch 53/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 10.0291 - mae: 1.8091 - val_loss: 10.2966 - val_mae: 2.2759\n",
            "Epoch 54/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 11.6392 - mae: 1.8599 - val_loss: 10.0120 - val_mae: 2.2626\n",
            "Epoch 55/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.9722 - mae: 1.6306 - val_loss: 12.7758 - val_mae: 2.4760\n",
            "Epoch 56/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6526 - mae: 1.3950 - val_loss: 11.1267 - val_mae: 2.4891\n",
            "Epoch 57/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.6204 - mae: 1.6736 - val_loss: 10.9843 - val_mae: 2.4188\n",
            "Epoch 58/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.3234 - mae: 1.3009 - val_loss: 12.3973 - val_mae: 2.4989\n",
            "Epoch 59/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.4555 - mae: 1.5995 - val_loss: 12.3952 - val_mae: 2.4653\n",
            "Epoch 60/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.5051 - mae: 1.3440 - val_loss: 10.8958 - val_mae: 2.3645\n",
            "Epoch 61/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.3681 - mae: 1.4402 - val_loss: 10.0188 - val_mae: 2.2446\n",
            "Epoch 62/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.4832 - mae: 1.2984 - val_loss: 12.0964 - val_mae: 2.5712\n",
            "Epoch 63/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.0810 - mae: 1.5190 - val_loss: 10.5335 - val_mae: 2.3753\n",
            "Epoch 64/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.3909 - mae: 1.4729 - val_loss: 11.2686 - val_mae: 2.3618\n",
            "Epoch 65/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 13.0753 - mae: 1.8591 - val_loss: 10.7554 - val_mae: 2.3173\n",
            "Epoch 66/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.4950 - mae: 1.6687 - val_loss: 11.5018 - val_mae: 2.4752\n",
            "Epoch 67/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.7949 - mae: 1.4174 - val_loss: 11.4664 - val_mae: 2.5372\n",
            "Epoch 68/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.5647 - mae: 1.6058 - val_loss: 10.8652 - val_mae: 2.3345\n",
            "Epoch 69/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.4699 - mae: 1.3317 - val_loss: 10.1491 - val_mae: 2.2859\n",
            "Epoch 70/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 4.8561 - mae: 1.4621 - val_loss: 11.4094 - val_mae: 2.4773\n",
            "Epoch 71/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.1346 - mae: 1.4109 - val_loss: 13.1985 - val_mae: 2.6220\n",
            "Epoch 72/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.3419 - mae: 1.5949 - val_loss: 11.6040 - val_mae: 2.4099\n",
            "Epoch 73/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.4488 - mae: 1.3298 - val_loss: 10.5421 - val_mae: 2.3389\n",
            "Epoch 74/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.2527 - mae: 1.4410 - val_loss: 11.4425 - val_mae: 2.4864\n",
            "Epoch 75/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.9723 - mae: 1.4401 - val_loss: 11.4015 - val_mae: 2.3769\n",
            "Epoch 76/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.4158 - mae: 1.2684 - val_loss: 11.6000 - val_mae: 2.4726\n",
            "Epoch 77/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.6316 - mae: 1.5601 - val_loss: 11.5903 - val_mae: 2.4270\n",
            "Epoch 78/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.7470 - mae: 1.4894 - val_loss: 12.5266 - val_mae: 2.5376\n",
            "Epoch 79/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.5223 - mae: 1.5418 - val_loss: 13.1113 - val_mae: 2.5876\n",
            "Epoch 80/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.3205 - mae: 1.4074 - val_loss: 10.7795 - val_mae: 2.2760\n",
            "Epoch 81/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.3920 - mae: 1.1721 - val_loss: 11.4615 - val_mae: 2.4164\n",
            "Epoch 82/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.8783 - mae: 1.3678 - val_loss: 11.2201 - val_mae: 2.4029\n",
            "Epoch 83/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.2844 - mae: 1.2958 - val_loss: 14.5108 - val_mae: 2.9102\n",
            "Epoch 84/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.2490 - mae: 1.5009 - val_loss: 12.0987 - val_mae: 2.4467\n",
            "Epoch 85/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.9857 - mae: 1.4599 - val_loss: 10.8562 - val_mae: 2.3534\n",
            "Epoch 86/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.3558 - mae: 1.2470 - val_loss: 10.7431 - val_mae: 2.3945\n",
            "Epoch 87/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 8.2355 - mae: 1.5907 - val_loss: 12.7485 - val_mae: 2.4793\n",
            "Epoch 88/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.8298 - mae: 1.4147 - val_loss: 11.6220 - val_mae: 2.5475\n",
            "Epoch 89/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.3090 - mae: 1.2765 - val_loss: 11.5901 - val_mae: 2.4991\n",
            "Epoch 90/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.6395 - mae: 1.3981 - val_loss: 11.3100 - val_mae: 2.4122\n",
            "Epoch 91/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.8109 - mae: 1.2880 - val_loss: 10.4894 - val_mae: 2.4042\n",
            "Epoch 92/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.0965 - mae: 1.3000 - val_loss: 11.5644 - val_mae: 2.4134\n",
            "Epoch 93/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6086 - mae: 1.2569 - val_loss: 10.7096 - val_mae: 2.3736\n",
            "Epoch 94/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.9235 - mae: 1.3423 - val_loss: 11.7177 - val_mae: 2.4417\n",
            "Epoch 95/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6840 - mae: 1.2578 - val_loss: 11.6367 - val_mae: 2.5204\n",
            "Epoch 96/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.7633 - mae: 1.1898 - val_loss: 11.9180 - val_mae: 2.5739\n",
            "Epoch 97/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.6781 - mae: 1.1592 - val_loss: 10.1582 - val_mae: 2.3333\n",
            "Epoch 98/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.0693 - mae: 1.1242 - val_loss: 11.1475 - val_mae: 2.4460\n",
            "Epoch 99/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.5779 - mae: 1.3860 - val_loss: 11.3135 - val_mae: 2.4649\n",
            "Epoch 100/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.2794 - mae: 1.3976 - val_loss: 11.3591 - val_mae: 2.4005\n",
            "Epoch 101/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.6341 - mae: 1.4565 - val_loss: 11.5492 - val_mae: 2.4466\n",
            "Epoch 102/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.8051 - mae: 1.2395 - val_loss: 12.0830 - val_mae: 2.4941\n",
            "Epoch 103/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.1663 - mae: 1.2639 - val_loss: 10.8274 - val_mae: 2.3871\n",
            "Epoch 104/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.9294 - mae: 1.2732 - val_loss: 13.9425 - val_mae: 2.7072\n",
            "Epoch 105/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.7744 - mae: 1.3402 - val_loss: 11.8746 - val_mae: 2.4898\n",
            "Epoch 106/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.6608 - mae: 1.1936 - val_loss: 14.2998 - val_mae: 2.8714\n",
            "Epoch 107/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.1364 - mae: 1.1978 - val_loss: 11.3035 - val_mae: 2.4072\n",
            "Epoch 108/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.5667 - mae: 1.0967 - val_loss: 12.9854 - val_mae: 2.6729\n",
            "Epoch 109/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.5964 - mae: 1.1519 - val_loss: 11.6554 - val_mae: 2.4419\n",
            "Epoch 110/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.0830 - mae: 1.2331 - val_loss: 12.6776 - val_mae: 2.6002\n",
            "Epoch 111/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.2075 - mae: 1.2379 - val_loss: 11.3633 - val_mae: 2.3954\n",
            "Epoch 112/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5003 - mae: 1.2116 - val_loss: 10.6888 - val_mae: 2.3816\n",
            "Epoch 113/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.6753 - mae: 1.2179 - val_loss: 10.4300 - val_mae: 2.3613\n",
            "Epoch 114/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.9554 - mae: 1.1603 - val_loss: 10.8830 - val_mae: 2.4003\n",
            "Epoch 115/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.1878 - mae: 1.1642 - val_loss: 11.4297 - val_mae: 2.4901\n",
            "Epoch 116/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.4388 - mae: 1.2083 - val_loss: 12.0052 - val_mae: 2.5595\n",
            "Epoch 117/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.3149 - mae: 1.2007 - val_loss: 10.4449 - val_mae: 2.3730\n",
            "Epoch 118/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.7392 - mae: 1.2853 - val_loss: 11.7351 - val_mae: 2.4921\n",
            "Epoch 119/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.0532 - mae: 1.2723 - val_loss: 10.8515 - val_mae: 2.3936\n",
            "Epoch 120/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2633 - mae: 1.0052 - val_loss: 10.3591 - val_mae: 2.2666\n",
            "Epoch 121/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3789 - mae: 1.0331 - val_loss: 12.3206 - val_mae: 2.5549\n",
            "Epoch 122/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.7339 - mae: 1.2356 - val_loss: 10.5536 - val_mae: 2.4173\n",
            "Epoch 123/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.1648 - mae: 1.1767 - val_loss: 11.1655 - val_mae: 2.3783\n",
            "Epoch 124/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.3236 - mae: 1.2321 - val_loss: 10.6869 - val_mae: 2.4662\n",
            "Epoch 125/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.3952 - mae: 1.1776 - val_loss: 11.7993 - val_mae: 2.4711\n",
            "Epoch 126/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.6043 - mae: 1.2068 - val_loss: 10.6568 - val_mae: 2.4078\n",
            "Epoch 127/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.2921 - mae: 1.3235 - val_loss: 10.2461 - val_mae: 2.2998\n",
            "Epoch 128/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.9641 - mae: 1.0705 - val_loss: 10.4725 - val_mae: 2.3805\n",
            "Epoch 129/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.8864 - mae: 1.1365 - val_loss: 9.7841 - val_mae: 2.2678\n",
            "Epoch 130/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1000 - mae: 1.0401 - val_loss: 11.6535 - val_mae: 2.4700\n",
            "Epoch 131/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.4927 - mae: 1.2276 - val_loss: 10.4321 - val_mae: 2.3475\n",
            "Epoch 132/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.9888 - mae: 0.9740 - val_loss: 11.0105 - val_mae: 2.4879\n",
            "Epoch 133/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.8886 - mae: 1.1523 - val_loss: 10.2358 - val_mae: 2.3358\n",
            "Epoch 134/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.9428 - mae: 1.1116 - val_loss: 13.4673 - val_mae: 2.7831\n",
            "Epoch 135/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.4026 - mae: 1.1497 - val_loss: 10.0026 - val_mae: 2.2898\n",
            "Epoch 136/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.4204 - mae: 1.1110 - val_loss: 10.4758 - val_mae: 2.3525\n",
            "Epoch 137/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.4580 - mae: 1.0123 - val_loss: 10.2403 - val_mae: 2.3506\n",
            "Epoch 138/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2558 - mae: 1.0606 - val_loss: 10.8162 - val_mae: 2.3364\n",
            "Epoch 139/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.7016 - mae: 1.1171 - val_loss: 10.3522 - val_mae: 2.3578\n",
            "Epoch 140/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.4632 - mae: 1.2118 - val_loss: 12.2496 - val_mae: 2.5340\n",
            "Epoch 141/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2413 - mae: 0.9910 - val_loss: 15.4080 - val_mae: 3.0028\n",
            "Epoch 142/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1615 - mae: 1.0661 - val_loss: 10.6382 - val_mae: 2.4762\n",
            "Epoch 143/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.7006 - mae: 1.1817 - val_loss: 12.0182 - val_mae: 2.5460\n",
            "Epoch 144/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.3388 - mae: 1.1328 - val_loss: 12.1857 - val_mae: 2.5622\n",
            "Epoch 145/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.3581 - mae: 1.1348 - val_loss: 12.7210 - val_mae: 2.5560\n",
            "Epoch 146/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.0865 - mae: 1.2102 - val_loss: 11.3131 - val_mae: 2.4164\n",
            "Epoch 147/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.8971 - mae: 0.9988 - val_loss: 11.3634 - val_mae: 2.4125\n",
            "Epoch 148/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.3518 - mae: 1.1339 - val_loss: 11.5518 - val_mae: 2.4409\n",
            "Epoch 149/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.4611 - mae: 1.2048 - val_loss: 10.7031 - val_mae: 2.4094\n",
            "Epoch 150/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.6153 - mae: 1.0586 - val_loss: 11.7650 - val_mae: 2.5040\n",
            "Epoch 151/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.6140 - mae: 1.0962 - val_loss: 10.9274 - val_mae: 2.3488\n",
            "Epoch 152/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.5638 - mae: 1.1942 - val_loss: 11.0482 - val_mae: 2.4263\n",
            "Epoch 153/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.0347 - mae: 1.1698 - val_loss: 10.8480 - val_mae: 2.3733\n",
            "Epoch 154/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0621 - mae: 1.0397 - val_loss: 12.5337 - val_mae: 2.6018\n",
            "Epoch 155/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6181 - mae: 1.1991 - val_loss: 12.4518 - val_mae: 2.6174\n",
            "Epoch 156/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.3709 - mae: 1.1664 - val_loss: 11.0764 - val_mae: 2.4095\n",
            "Epoch 157/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.3500 - mae: 1.2417 - val_loss: 11.9103 - val_mae: 2.4829\n",
            "Epoch 158/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.8642 - mae: 1.1108 - val_loss: 11.1674 - val_mae: 2.3580\n",
            "Epoch 159/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.3106 - mae: 1.0619 - val_loss: 11.0286 - val_mae: 2.3343\n",
            "Epoch 160/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3472 - mae: 1.0129 - val_loss: 10.4835 - val_mae: 2.3519\n",
            "Epoch 161/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.9283 - mae: 1.1165 - val_loss: 10.1517 - val_mae: 2.2978\n",
            "Epoch 162/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3364 - mae: 1.0256 - val_loss: 10.8938 - val_mae: 2.3818\n",
            "Epoch 163/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.6963 - mae: 1.1296 - val_loss: 10.7174 - val_mae: 2.4096\n",
            "Epoch 164/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.3398 - mae: 1.0807 - val_loss: 11.5665 - val_mae: 2.4242\n",
            "Epoch 165/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.1677 - mae: 1.1669 - val_loss: 11.8852 - val_mae: 2.5388\n",
            "Epoch 166/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.7835 - mae: 1.1736 - val_loss: 10.7642 - val_mae: 2.3011\n",
            "Epoch 167/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.6362 - mae: 1.0380 - val_loss: 10.9299 - val_mae: 2.4299\n",
            "Epoch 168/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.9123 - mae: 1.0545 - val_loss: 11.2066 - val_mae: 2.4656\n",
            "Epoch 169/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.4545 - mae: 0.9444 - val_loss: 12.2271 - val_mae: 2.5097\n",
            "Epoch 170/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3846 - mae: 1.0806 - val_loss: 11.0491 - val_mae: 2.4452\n",
            "Epoch 171/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.8759 - mae: 0.9790 - val_loss: 14.5183 - val_mae: 2.7998\n",
            "Epoch 172/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.5402 - mae: 1.1270 - val_loss: 10.2555 - val_mae: 2.3434\n",
            "Epoch 173/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1148 - mae: 0.9958 - val_loss: 15.5476 - val_mae: 3.0397\n",
            "Epoch 174/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.7162 - mae: 1.1393 - val_loss: 10.7470 - val_mae: 2.3614\n",
            "Epoch 175/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.0776 - mae: 1.0770 - val_loss: 10.5181 - val_mae: 2.3256\n",
            "Epoch 176/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.8168 - mae: 1.1043 - val_loss: 10.0630 - val_mae: 2.2936\n",
            "Epoch 177/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.2545 - mae: 1.0297 - val_loss: 10.8274 - val_mae: 2.3995\n",
            "Epoch 178/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.6325 - mae: 1.0800 - val_loss: 10.2395 - val_mae: 2.3481\n",
            "Epoch 179/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.8347 - mae: 1.0694 - val_loss: 9.4240 - val_mae: 2.2314\n",
            "Epoch 180/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.9181 - mae: 0.9840 - val_loss: 11.1275 - val_mae: 2.4213\n",
            "Epoch 181/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.3851 - mae: 1.0847 - val_loss: 10.6634 - val_mae: 2.3493\n",
            "Epoch 182/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.1346 - mae: 0.9463 - val_loss: 10.9878 - val_mae: 2.3975\n",
            "Epoch 183/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.7096 - mae: 1.0839 - val_loss: 12.1069 - val_mae: 2.5482\n",
            "Epoch 184/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.7546 - mae: 0.9137 - val_loss: 10.9277 - val_mae: 2.4288\n",
            "Epoch 185/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.8834 - mae: 0.9289 - val_loss: 10.2315 - val_mae: 2.3535\n",
            "Epoch 186/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.8253 - mae: 1.0414 - val_loss: 11.4845 - val_mae: 2.4451\n",
            "Epoch 187/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.7319 - mae: 0.8812 - val_loss: 11.4722 - val_mae: 2.4834\n",
            "Epoch 188/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.7675 - mae: 0.9355 - val_loss: 11.9417 - val_mae: 2.5470\n",
            "Epoch 189/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.7785 - mae: 1.0993 - val_loss: 11.0596 - val_mae: 2.3688\n",
            "Epoch 190/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.8359 - mae: 0.9803 - val_loss: 11.1750 - val_mae: 2.5384\n",
            "Epoch 191/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.3377 - mae: 1.0280 - val_loss: 10.8027 - val_mae: 2.3688\n",
            "Epoch 192/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.9472 - mae: 1.1146 - val_loss: 11.4626 - val_mae: 2.4508\n",
            "Epoch 193/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.9301 - mae: 0.8990 - val_loss: 9.8863 - val_mae: 2.3188\n",
            "Epoch 194/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.8720 - mae: 0.9431 - val_loss: 11.1946 - val_mae: 2.4513\n",
            "Epoch 195/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.8068 - mae: 1.0630 - val_loss: 11.4470 - val_mae: 2.4672\n",
            "Epoch 196/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.6979 - mae: 0.9983 - val_loss: 10.1102 - val_mae: 2.3263\n",
            "Epoch 197/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0456 - mae: 0.9460 - val_loss: 11.4925 - val_mae: 2.4236\n",
            "Epoch 198/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.8847 - mae: 0.8927 - val_loss: 10.2448 - val_mae: 2.3051\n",
            "Epoch 199/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.9488 - mae: 0.9136 - val_loss: 10.5138 - val_mae: 2.3578\n",
            "Epoch 200/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.4517 - mae: 0.9943 - val_loss: 11.5440 - val_mae: 2.4232\n",
            "Epoch 201/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0831 - mae: 1.0143 - val_loss: 12.1523 - val_mae: 2.5023\n",
            "Epoch 202/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.6286 - mae: 0.9287 - val_loss: 10.2348 - val_mae: 2.3353\n",
            "Epoch 203/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2550 - mae: 1.0237 - val_loss: 10.5672 - val_mae: 2.4108\n",
            "Epoch 204/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0639 - mae: 0.9034 - val_loss: 10.7598 - val_mae: 2.3757\n",
            "Epoch 205/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.9990 - mae: 0.8859 - val_loss: 11.3948 - val_mae: 2.4458\n",
            "Epoch 206/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.1748 - mae: 1.0064 - val_loss: 10.9320 - val_mae: 2.3713\n",
            "Epoch 207/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.0216 - mae: 0.9615 - val_loss: 11.4645 - val_mae: 2.4785\n",
            "Epoch 208/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.8627 - mae: 0.9330 - val_loss: 11.5155 - val_mae: 2.4827\n",
            "Epoch 209/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1120 - mae: 0.9842 - val_loss: 10.2816 - val_mae: 2.3844\n",
            "Epoch 210/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0136 - mae: 0.9827 - val_loss: 9.8144 - val_mae: 2.3165\n",
            "Epoch 211/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.3541 - mae: 1.1186 - val_loss: 10.7273 - val_mae: 2.4103\n",
            "Epoch 212/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.5777 - mae: 1.0547 - val_loss: 12.8544 - val_mae: 2.6505\n",
            "Epoch 213/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.6859 - mae: 0.8933 - val_loss: 11.2490 - val_mae: 2.5254\n",
            "Epoch 214/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.1211 - mae: 1.1586 - val_loss: 10.1706 - val_mae: 2.2920\n",
            "Epoch 215/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0055 - mae: 0.9558 - val_loss: 11.1082 - val_mae: 2.4985\n",
            "Epoch 216/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.6488 - mae: 1.1302 - val_loss: 14.6804 - val_mae: 2.9053\n",
            "Epoch 217/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.5594 - mae: 1.0020 - val_loss: 10.0137 - val_mae: 2.3941\n",
            "Epoch 218/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.5257 - mae: 0.8750 - val_loss: 11.0694 - val_mae: 2.4549\n",
            "Epoch 219/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 1.9501 - mae: 0.9385 - val_loss: 11.2762 - val_mae: 2.4325\n",
            "Epoch 220/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.8828 - mae: 0.9265 - val_loss: 10.7704 - val_mae: 2.4268\n",
            "Epoch 221/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.0241 - mae: 0.9574 - val_loss: 10.4962 - val_mae: 2.4189\n",
            "Epoch 222/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.9138 - mae: 0.9762 - val_loss: 10.2505 - val_mae: 2.3846\n",
            "Epoch 223/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.7476 - mae: 0.9153 - val_loss: 10.1203 - val_mae: 2.3866\n",
            "Epoch 224/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.6328 - mae: 1.0080 - val_loss: 12.1384 - val_mae: 2.5902\n",
            "Epoch 225/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3076 - mae: 1.0550 - val_loss: 9.7710 - val_mae: 2.2850\n",
            "Epoch 226/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1954 - mae: 0.9625 - val_loss: 10.1366 - val_mae: 2.3150\n",
            "Epoch 227/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1044 - mae: 0.9676 - val_loss: 11.5395 - val_mae: 2.5193\n",
            "Epoch 228/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.7308 - mae: 0.8980 - val_loss: 11.3177 - val_mae: 2.4365\n",
            "Epoch 229/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2309 - mae: 1.0478 - val_loss: 13.3737 - val_mae: 2.6911\n",
            "Epoch 230/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0134 - mae: 0.9978 - val_loss: 12.2425 - val_mae: 2.5528\n",
            "Epoch 231/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.0862 - mae: 0.9997 - val_loss: 11.5696 - val_mae: 2.5220\n",
            "Epoch 232/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.8208 - mae: 1.0207 - val_loss: 12.1818 - val_mae: 2.5411\n",
            "Epoch 233/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.1111 - mae: 0.9359 - val_loss: 11.7550 - val_mae: 2.5687\n",
            "Epoch 234/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.2837 - mae: 1.0353 - val_loss: 10.0363 - val_mae: 2.2905\n",
            "Epoch 235/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.6446 - mae: 0.9496 - val_loss: 11.8487 - val_mae: 2.5460\n",
            "Epoch 236/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.9828 - mae: 0.9695 - val_loss: 10.5168 - val_mae: 2.3432\n",
            "Epoch 237/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.5234 - mae: 0.8308 - val_loss: 15.8995 - val_mae: 2.9243\n",
            "Epoch 238/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.7366 - mae: 0.9191 - val_loss: 12.5468 - val_mae: 2.5757\n",
            "Epoch 239/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.8355 - mae: 0.9150 - val_loss: 12.3824 - val_mae: 2.5591\n",
            "Epoch 240/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.5913 - mae: 0.8850 - val_loss: 13.1890 - val_mae: 2.6508\n",
            "Epoch 241/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.4419 - mae: 0.9225 - val_loss: 11.3605 - val_mae: 2.4672\n",
            "Epoch 242/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3768 - mae: 0.9686 - val_loss: 11.0606 - val_mae: 2.4020\n",
            "Epoch 243/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0098 - mae: 0.9375 - val_loss: 12.2354 - val_mae: 2.5447\n",
            "Epoch 244/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.7777 - mae: 0.9371 - val_loss: 11.3187 - val_mae: 2.4792\n",
            "Epoch 245/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0555 - mae: 0.9765 - val_loss: 11.5255 - val_mae: 2.4074\n",
            "Epoch 246/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.8378 - mae: 1.0780 - val_loss: 11.7212 - val_mae: 2.4903\n",
            "Epoch 247/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.7861 - mae: 0.9301 - val_loss: 12.1486 - val_mae: 2.5456\n",
            "Epoch 248/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.1378 - mae: 0.9890 - val_loss: 11.1195 - val_mae: 2.4134\n",
            "Epoch 249/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.8130 - mae: 0.9382 - val_loss: 12.0307 - val_mae: 2.4610\n",
            "Epoch 250/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.9758 - mae: 0.9536 - val_loss: 11.8808 - val_mae: 2.4717\n",
            "Epoch 251/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.4793 - mae: 0.9999 - val_loss: 14.1930 - val_mae: 2.7248\n",
            "Epoch 252/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.6770 - mae: 0.9671 - val_loss: 9.6559 - val_mae: 2.2392\n",
            "Epoch 253/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.6706 - mae: 0.9156 - val_loss: 11.1097 - val_mae: 2.4967\n",
            "Epoch 254/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.8541 - mae: 0.9762 - val_loss: 13.4353 - val_mae: 2.6758\n",
            "Epoch 255/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4258 - mae: 0.8151 - val_loss: 10.2384 - val_mae: 2.3465\n",
            "Epoch 256/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1812 - mae: 1.0070 - val_loss: 12.3305 - val_mae: 2.6121\n",
            "Epoch 257/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4895 - mae: 0.8296 - val_loss: 10.4872 - val_mae: 2.3093\n",
            "Epoch 258/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3308 - mae: 0.8179 - val_loss: 12.0156 - val_mae: 2.4662\n",
            "Epoch 259/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.1688 - mae: 1.0688 - val_loss: 11.1076 - val_mae: 2.3881\n",
            "Epoch 260/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.0241 - mae: 1.0183 - val_loss: 10.6928 - val_mae: 2.3811\n",
            "Epoch 261/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.5856 - mae: 0.9064 - val_loss: 12.0035 - val_mae: 2.5099\n",
            "Epoch 262/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1500 - mae: 0.9442 - val_loss: 12.9541 - val_mae: 2.5345\n",
            "Epoch 263/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.7561 - mae: 0.9234 - val_loss: 11.6046 - val_mae: 2.4843\n",
            "Epoch 264/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0123 - mae: 0.9890 - val_loss: 11.0604 - val_mae: 2.4739\n",
            "Epoch 265/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.9270 - mae: 0.9937 - val_loss: 11.7736 - val_mae: 2.5267\n",
            "Epoch 266/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.6933 - mae: 0.9098 - val_loss: 11.6005 - val_mae: 2.5349\n",
            "Epoch 267/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4400 - mae: 0.8601 - val_loss: 12.4375 - val_mae: 2.5281\n",
            "Epoch 268/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.9352 - mae: 0.8858 - val_loss: 11.9692 - val_mae: 2.5088\n",
            "Epoch 269/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.8929 - mae: 0.9668 - val_loss: 12.4645 - val_mae: 2.5672\n",
            "Epoch 270/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 1.4708 - mae: 0.8904 - val_loss: 11.6852 - val_mae: 2.4928\n",
            "Epoch 271/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.9058 - mae: 1.0051 - val_loss: 11.9082 - val_mae: 2.5846\n",
            "Epoch 272/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.5046 - mae: 0.8677 - val_loss: 12.9502 - val_mae: 2.5969\n",
            "Epoch 273/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.9057 - mae: 1.0376 - val_loss: 11.4236 - val_mae: 2.4375\n",
            "Epoch 274/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.9363 - mae: 0.9494 - val_loss: 11.8916 - val_mae: 2.5083\n",
            "Epoch 275/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2937 - mae: 0.8199 - val_loss: 9.6244 - val_mae: 2.3043\n",
            "Epoch 276/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.5389 - mae: 0.8580 - val_loss: 13.4053 - val_mae: 2.6895\n",
            "Epoch 277/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.5171 - mae: 0.8785 - val_loss: 12.0095 - val_mae: 2.4996\n",
            "Epoch 278/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.6204 - mae: 0.9150 - val_loss: 13.1041 - val_mae: 2.6002\n",
            "Epoch 279/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4262 - mae: 0.8836 - val_loss: 12.4329 - val_mae: 2.5824\n",
            "Epoch 280/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1044 - mae: 0.9310 - val_loss: 11.5388 - val_mae: 2.4967\n",
            "Epoch 281/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2349 - mae: 0.9685 - val_loss: 12.4006 - val_mae: 2.6063\n",
            "Epoch 282/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.6620 - mae: 0.9606 - val_loss: 12.1972 - val_mae: 2.4931\n",
            "Epoch 283/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.5430 - mae: 0.8741 - val_loss: 11.6603 - val_mae: 2.5148\n",
            "Epoch 284/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4795 - mae: 0.8533 - val_loss: 12.3814 - val_mae: 2.5736\n",
            "Epoch 285/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.5370 - mae: 0.8839 - val_loss: 12.4797 - val_mae: 2.5920\n",
            "Epoch 286/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.3725 - mae: 0.8828 - val_loss: 15.7626 - val_mae: 2.9884\n",
            "Epoch 287/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.5645 - mae: 0.8502 - val_loss: 11.5044 - val_mae: 2.4945\n",
            "Epoch 288/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.2765 - mae: 0.8254 - val_loss: 13.8364 - val_mae: 2.7485\n",
            "Epoch 289/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.6120 - mae: 0.9078 - val_loss: 12.8663 - val_mae: 2.6160\n",
            "Epoch 290/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4942 - mae: 0.8746 - val_loss: 12.6446 - val_mae: 2.5813\n",
            "Epoch 291/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.5100 - mae: 0.8998 - val_loss: 11.8116 - val_mae: 2.4719\n",
            "Epoch 292/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.6666 - mae: 0.8585 - val_loss: 10.5851 - val_mae: 2.3636\n",
            "Epoch 293/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2196 - mae: 0.9937 - val_loss: 11.8437 - val_mae: 2.5372\n",
            "Epoch 294/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.6880 - mae: 0.8631 - val_loss: 14.2806 - val_mae: 2.7285\n",
            "Epoch 295/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2666 - mae: 0.8043 - val_loss: 13.1420 - val_mae: 2.6611\n",
            "Epoch 296/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.6968 - mae: 0.9045 - val_loss: 14.1025 - val_mae: 2.7792\n",
            "Epoch 297/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2974 - mae: 0.8143 - val_loss: 12.9732 - val_mae: 2.5953\n",
            "Epoch 298/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.8213 - mae: 0.9551 - val_loss: 12.4277 - val_mae: 2.5301\n",
            "Epoch 299/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1.1110 - mae: 0.7183 - val_loss: 13.3070 - val_mae: 2.5598\n",
            "Epoch 300/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.2929 - mae: 0.7974 - val_loss: 12.8249 - val_mae: 2.5951\n",
            "Epoch 301/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.4726 - mae: 0.8663 - val_loss: 13.0549 - val_mae: 2.6155\n",
            "Epoch 302/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3176 - mae: 0.8369 - val_loss: 12.5379 - val_mae: 2.5765\n",
            "Epoch 303/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.7556 - mae: 0.8136 - val_loss: 10.8008 - val_mae: 2.3667\n",
            "Epoch 304/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4169 - mae: 0.8958 - val_loss: 13.1275 - val_mae: 2.6363\n",
            "Epoch 305/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2308 - mae: 0.8359 - val_loss: 11.8990 - val_mae: 2.5084\n",
            "Epoch 306/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2789 - mae: 0.8437 - val_loss: 12.5703 - val_mae: 2.5096\n",
            "Epoch 307/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2014 - mae: 0.7866 - val_loss: 13.1285 - val_mae: 2.6168\n",
            "Epoch 308/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3206 - mae: 0.8436 - val_loss: 12.1176 - val_mae: 2.5465\n",
            "Epoch 309/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.6326 - mae: 0.9011 - val_loss: 11.9701 - val_mae: 2.5451\n",
            "Epoch 310/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4553 - mae: 0.8888 - val_loss: 13.0231 - val_mae: 2.5752\n",
            "Epoch 311/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3503 - mae: 0.8338 - val_loss: 14.4744 - val_mae: 2.7632\n",
            "Epoch 312/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3220 - mae: 0.7706 - val_loss: 12.9074 - val_mae: 2.5943\n",
            "Epoch 313/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4073 - mae: 0.8307 - val_loss: 11.9636 - val_mae: 2.5078\n",
            "Epoch 314/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.3336 - mae: 0.8360 - val_loss: 12.2812 - val_mae: 2.5957\n",
            "Epoch 315/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.5991 - mae: 0.9094 - val_loss: 12.6011 - val_mae: 2.5743\n",
            "Epoch 316/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.7091 - mae: 0.8983 - val_loss: 10.9825 - val_mae: 2.3816\n",
            "Epoch 317/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0866 - mae: 0.7608 - val_loss: 14.9547 - val_mae: 2.7859\n",
            "Epoch 318/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.6073 - mae: 0.8774 - val_loss: 12.2545 - val_mae: 2.5382\n",
            "Epoch 319/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.6629 - mae: 0.9202 - val_loss: 13.3457 - val_mae: 2.6356\n",
            "Epoch 320/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3298 - mae: 0.7917 - val_loss: 11.9165 - val_mae: 2.4919\n",
            "Epoch 321/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.5595 - mae: 0.8191 - val_loss: 12.0303 - val_mae: 2.4980\n",
            "Epoch 322/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4522 - mae: 0.8666 - val_loss: 12.4766 - val_mae: 2.5604\n",
            "Epoch 323/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.9688 - mae: 0.9894 - val_loss: 13.4655 - val_mae: 2.6332\n",
            "Epoch 324/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2328 - mae: 0.7598 - val_loss: 14.8756 - val_mae: 2.8130\n",
            "Epoch 325/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1698 - mae: 0.8093 - val_loss: 12.9173 - val_mae: 2.6414\n",
            "Epoch 326/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.5350 - mae: 0.9121 - val_loss: 12.7646 - val_mae: 2.5861\n",
            "Epoch 327/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0918 - mae: 0.7326 - val_loss: 13.8659 - val_mae: 2.7565\n",
            "Epoch 328/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2076 - mae: 0.8193 - val_loss: 13.2315 - val_mae: 2.6141\n",
            "Epoch 329/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.4777 - mae: 0.8858 - val_loss: 14.1471 - val_mae: 2.7264\n",
            "Epoch 330/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.5070 - mae: 0.8500 - val_loss: 14.2807 - val_mae: 2.7845\n",
            "Epoch 331/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.4660 - mae: 0.8458 - val_loss: 13.8417 - val_mae: 2.6924\n",
            "Epoch 332/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4789 - mae: 0.8160 - val_loss: 13.0869 - val_mae: 2.6023\n",
            "Epoch 333/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3049 - mae: 0.7591 - val_loss: 14.8045 - val_mae: 2.7991\n",
            "Epoch 334/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.5595 - mae: 0.8292 - val_loss: 16.3824 - val_mae: 3.0333\n",
            "Epoch 335/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.5820 - mae: 0.9294 - val_loss: 12.5810 - val_mae: 2.5628\n",
            "Epoch 336/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1800 - mae: 0.7096 - val_loss: 12.9034 - val_mae: 2.6144\n",
            "Epoch 337/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4084 - mae: 0.7878 - val_loss: 13.7153 - val_mae: 2.6651\n",
            "Epoch 338/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4179 - mae: 0.8562 - val_loss: 11.8322 - val_mae: 2.4780\n",
            "Epoch 339/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3536 - mae: 0.8455 - val_loss: 12.7367 - val_mae: 2.6294\n",
            "Epoch 340/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4019 - mae: 0.8640 - val_loss: 14.5709 - val_mae: 2.8315\n",
            "Epoch 341/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1394 - mae: 0.7762 - val_loss: 12.7901 - val_mae: 2.6162\n",
            "Epoch 342/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2822 - mae: 0.7795 - val_loss: 12.0071 - val_mae: 2.5120\n",
            "Epoch 343/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3492 - mae: 0.7924 - val_loss: 16.1306 - val_mae: 2.9577\n",
            "Epoch 344/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1.2029 - mae: 0.8021 - val_loss: 14.8080 - val_mae: 2.7661\n",
            "Epoch 345/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.1937 - mae: 0.8173 - val_loss: 14.1233 - val_mae: 2.7181\n",
            "Epoch 346/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.4224 - mae: 0.8048 - val_loss: 14.0167 - val_mae: 2.6435\n",
            "Epoch 347/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3844 - mae: 0.8697 - val_loss: 13.0384 - val_mae: 2.5718\n",
            "Epoch 348/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3346 - mae: 0.7910 - val_loss: 14.2238 - val_mae: 2.6751\n",
            "Epoch 349/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.5981 - mae: 0.8670 - val_loss: 15.6607 - val_mae: 2.8645\n",
            "Epoch 350/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3200 - mae: 0.8241 - val_loss: 12.7238 - val_mae: 2.6214\n",
            "Epoch 351/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3252 - mae: 0.8126 - val_loss: 12.8726 - val_mae: 2.6267\n",
            "Epoch 352/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1897 - mae: 0.7848 - val_loss: 14.0997 - val_mae: 2.7369\n",
            "Epoch 353/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2682 - mae: 0.8167 - val_loss: 14.6209 - val_mae: 2.7880\n",
            "Epoch 354/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1929 - mae: 0.7990 - val_loss: 12.9169 - val_mae: 2.6486\n",
            "Epoch 355/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1562 - mae: 0.7803 - val_loss: 13.0488 - val_mae: 2.6754\n",
            "Epoch 356/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3523 - mae: 0.8467 - val_loss: 15.4386 - val_mae: 2.8877\n",
            "Epoch 357/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1033 - mae: 0.7438 - val_loss: 14.4057 - val_mae: 2.8188\n",
            "Epoch 358/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2121 - mae: 0.7603 - val_loss: 13.9892 - val_mae: 2.7210\n",
            "Epoch 359/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1.2598 - mae: 0.8424 - val_loss: 13.8011 - val_mae: 2.6543\n",
            "Epoch 360/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.4901 - mae: 0.8904 - val_loss: 13.6114 - val_mae: 2.6250\n",
            "Epoch 361/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.1370 - mae: 0.7737 - val_loss: 13.4302 - val_mae: 2.7171\n",
            "Epoch 362/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1599 - mae: 0.7970 - val_loss: 12.9740 - val_mae: 2.6369\n",
            "Epoch 363/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0785 - mae: 0.7595 - val_loss: 13.3089 - val_mae: 2.5935\n",
            "Epoch 364/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4854 - mae: 0.9061 - val_loss: 15.4921 - val_mae: 2.8464\n",
            "Epoch 365/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1942 - mae: 0.7659 - val_loss: 14.4123 - val_mae: 2.6593\n",
            "Epoch 366/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4723 - mae: 0.8544 - val_loss: 14.0501 - val_mae: 2.7098\n",
            "Epoch 367/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3011 - mae: 0.8367 - val_loss: 15.2019 - val_mae: 2.7723\n",
            "Epoch 368/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2597 - mae: 0.7766 - val_loss: 15.4631 - val_mae: 2.8286\n",
            "Epoch 369/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1552 - mae: 0.7970 - val_loss: 13.3676 - val_mae: 2.6929\n",
            "Epoch 370/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2282 - mae: 0.7999 - val_loss: 15.4459 - val_mae: 2.8184\n",
            "Epoch 371/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2820 - mae: 0.7675 - val_loss: 14.8838 - val_mae: 2.6983\n",
            "Epoch 372/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0985 - mae: 0.7730 - val_loss: 13.7695 - val_mae: 2.6282\n",
            "Epoch 373/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2628 - mae: 0.8030 - val_loss: 14.2108 - val_mae: 2.7427\n",
            "Epoch 374/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3044 - mae: 0.8300 - val_loss: 14.5840 - val_mae: 2.7046\n",
            "Epoch 375/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 1.1821 - mae: 0.7414 - val_loss: 14.5164 - val_mae: 2.7603\n",
            "Epoch 376/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.2370 - mae: 0.7710 - val_loss: 14.0581 - val_mae: 2.6956\n",
            "Epoch 377/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3164 - mae: 0.7638 - val_loss: 13.6138 - val_mae: 2.6301\n",
            "Epoch 378/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2019 - mae: 0.8081 - val_loss: 13.3959 - val_mae: 2.6075\n",
            "Epoch 379/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.8561 - mae: 0.6907 - val_loss: 17.4753 - val_mae: 2.9336\n",
            "Epoch 380/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1973 - mae: 0.7717 - val_loss: 15.9791 - val_mae: 2.9168\n",
            "Epoch 381/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1870 - mae: 0.7832 - val_loss: 14.7861 - val_mae: 2.7348\n",
            "Epoch 382/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.8997 - mae: 0.6554 - val_loss: 14.8323 - val_mae: 2.8481\n",
            "Epoch 383/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.7117 - mae: 0.9215 - val_loss: 14.3472 - val_mae: 2.7318\n",
            "Epoch 384/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2052 - mae: 0.8384 - val_loss: 15.3547 - val_mae: 2.8977\n",
            "Epoch 385/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3867 - mae: 0.8804 - val_loss: 14.4106 - val_mae: 2.7561\n",
            "Epoch 386/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3249 - mae: 0.8255 - val_loss: 14.7038 - val_mae: 2.7178\n",
            "Epoch 387/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9624 - mae: 0.7030 - val_loss: 17.2485 - val_mae: 3.0241\n",
            "Epoch 388/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.0507 - mae: 0.7610 - val_loss: 17.9607 - val_mae: 3.0725\n",
            "Epoch 389/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.1508 - mae: 0.7817 - val_loss: 13.9349 - val_mae: 2.6956\n",
            "Epoch 390/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.0145 - mae: 0.7442 - val_loss: 15.6593 - val_mae: 2.7872\n",
            "Epoch 391/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0727 - mae: 0.7497 - val_loss: 19.2798 - val_mae: 3.2029\n",
            "Epoch 392/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1549 - mae: 0.7926 - val_loss: 13.7991 - val_mae: 2.7575\n",
            "Epoch 393/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1297 - mae: 0.7836 - val_loss: 14.3253 - val_mae: 2.7859\n",
            "Epoch 394/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0780 - mae: 0.7574 - val_loss: 14.5069 - val_mae: 2.7750\n",
            "Epoch 395/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0533 - mae: 0.7357 - val_loss: 17.3639 - val_mae: 3.1229\n",
            "Epoch 396/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2135 - mae: 0.8324 - val_loss: 13.7292 - val_mae: 2.7298\n",
            "Epoch 397/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1174 - mae: 0.7641 - val_loss: 16.4634 - val_mae: 2.9734\n",
            "Epoch 398/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0232 - mae: 0.7326 - val_loss: 14.5796 - val_mae: 2.7582\n",
            "Epoch 399/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0949 - mae: 0.7653 - val_loss: 15.6268 - val_mae: 2.9141\n",
            "Epoch 400/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0253 - mae: 0.7507 - val_loss: 14.3368 - val_mae: 2.7166\n",
            "Epoch 401/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9472 - mae: 0.7103 - val_loss: 15.5681 - val_mae: 2.8703\n",
            "Epoch 402/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 1.2005 - mae: 0.7993 - val_loss: 14.5732 - val_mae: 2.7079\n",
            "Epoch 403/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.2127 - mae: 0.8136 - val_loss: 13.7270 - val_mae: 2.6792\n",
            "Epoch 404/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0286 - mae: 0.7608 - val_loss: 15.6495 - val_mae: 2.8766\n",
            "Epoch 405/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.1152 - mae: 0.7624 - val_loss: 14.5855 - val_mae: 2.7664\n",
            "Epoch 406/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1015 - mae: 0.7631 - val_loss: 14.9828 - val_mae: 2.7776\n",
            "Epoch 407/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1913 - mae: 0.8192 - val_loss: 14.0988 - val_mae: 2.7363\n",
            "Epoch 408/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0034 - mae: 0.6865 - val_loss: 13.9974 - val_mae: 2.6737\n",
            "Epoch 409/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0546 - mae: 0.7543 - val_loss: 14.7150 - val_mae: 2.8162\n",
            "Epoch 410/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0533 - mae: 0.7392 - val_loss: 13.5822 - val_mae: 2.6452\n",
            "Epoch 411/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2633 - mae: 0.7854 - val_loss: 17.1573 - val_mae: 2.9610\n",
            "Epoch 412/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1766 - mae: 0.7715 - val_loss: 16.1705 - val_mae: 2.8474\n",
            "Epoch 413/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0288 - mae: 0.7155 - val_loss: 17.0752 - val_mae: 2.9880\n",
            "Epoch 414/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2851 - mae: 0.8161 - val_loss: 13.5209 - val_mae: 2.6543\n",
            "Epoch 415/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1004 - mae: 0.7788 - val_loss: 14.3657 - val_mae: 2.7289\n",
            "Epoch 416/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1.0176 - mae: 0.7063 - val_loss: 16.4884 - val_mae: 2.8417\n",
            "Epoch 417/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9894 - mae: 0.7665 - val_loss: 14.8210 - val_mae: 2.8264\n",
            "Epoch 418/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.1245 - mae: 0.7395 - val_loss: 14.4041 - val_mae: 2.6870\n",
            "Epoch 419/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9130 - mae: 0.7084 - val_loss: 15.1848 - val_mae: 2.7676\n",
            "Epoch 420/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.8673 - mae: 0.7001 - val_loss: 13.6667 - val_mae: 2.7694\n",
            "Epoch 421/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4806 - mae: 0.8484 - val_loss: 14.3808 - val_mae: 2.6884\n",
            "Epoch 422/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9120 - mae: 0.7196 - val_loss: 18.0119 - val_mae: 3.0030\n",
            "Epoch 423/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2207 - mae: 0.7516 - val_loss: 15.2535 - val_mae: 2.8228\n",
            "Epoch 424/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3289 - mae: 0.8255 - val_loss: 14.7102 - val_mae: 2.7010\n",
            "Epoch 425/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2864 - mae: 0.8080 - val_loss: 14.6041 - val_mae: 2.7181\n",
            "Epoch 426/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1836 - mae: 0.7871 - val_loss: 15.2202 - val_mae: 2.7476\n",
            "Epoch 427/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.8586 - mae: 0.6498 - val_loss: 16.1310 - val_mae: 2.8226\n",
            "Epoch 428/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.1778 - mae: 0.7524 - val_loss: 13.3926 - val_mae: 2.6138\n",
            "Epoch 429/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.1403 - mae: 0.7598 - val_loss: 16.4688 - val_mae: 2.8503\n",
            "Epoch 430/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1431 - mae: 0.7555 - val_loss: 14.7286 - val_mae: 2.7375\n",
            "Epoch 431/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0337 - mae: 0.7558 - val_loss: 16.7681 - val_mae: 2.9460\n",
            "Epoch 432/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0268 - mae: 0.7179 - val_loss: 15.8515 - val_mae: 2.8565\n",
            "Epoch 433/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9837 - mae: 0.7313 - val_loss: 13.6983 - val_mae: 2.6338\n",
            "Epoch 434/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0326 - mae: 0.7607 - val_loss: 15.2923 - val_mae: 2.8265\n",
            "Epoch 435/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.8797 - mae: 0.6739 - val_loss: 14.5977 - val_mae: 2.6766\n",
            "Epoch 436/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9460 - mae: 0.7152 - val_loss: 15.5047 - val_mae: 2.7955\n",
            "Epoch 437/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9725 - mae: 0.6997 - val_loss: 14.0553 - val_mae: 2.6780\n",
            "Epoch 438/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1.1394 - mae: 0.7800 - val_loss: 14.6302 - val_mae: 2.7165\n",
            "Epoch 439/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.0165 - mae: 0.7630 - val_loss: 14.4861 - val_mae: 2.6904\n",
            "Epoch 440/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9895 - mae: 0.7404 - val_loss: 15.6279 - val_mae: 2.7760\n",
            "Epoch 441/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0270 - mae: 0.7434 - val_loss: 15.6375 - val_mae: 2.8874\n",
            "Epoch 442/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.8293 - mae: 0.6829 - val_loss: 13.6726 - val_mae: 2.6192\n",
            "Epoch 443/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0120 - mae: 0.7195 - val_loss: 15.6664 - val_mae: 2.8070\n",
            "Epoch 444/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9590 - mae: 0.7234 - val_loss: 17.2705 - val_mae: 2.9105\n",
            "Epoch 445/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0929 - mae: 0.7773 - val_loss: 15.3916 - val_mae: 2.7819\n",
            "Epoch 446/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9525 - mae: 0.6894 - val_loss: 15.8427 - val_mae: 2.8069\n",
            "Epoch 447/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4664 - mae: 0.7651 - val_loss: 14.4283 - val_mae: 2.6884\n",
            "Epoch 448/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9599 - mae: 0.7123 - val_loss: 16.1187 - val_mae: 2.9123\n",
            "Epoch 449/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1141 - mae: 0.7910 - val_loss: 13.9912 - val_mae: 2.6225\n",
            "Epoch 450/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.8720 - mae: 0.6512 - val_loss: 15.3915 - val_mae: 2.7681\n",
            "Epoch 451/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9014 - mae: 0.6916 - val_loss: 17.4233 - val_mae: 2.9856\n",
            "Epoch 452/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.2823 - mae: 0.8354 - val_loss: 15.5188 - val_mae: 2.7614\n",
            "Epoch 453/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.1287 - mae: 0.7337 - val_loss: 14.0874 - val_mae: 2.6234\n",
            "Epoch 454/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.8603 - mae: 0.7069 - val_loss: 15.1587 - val_mae: 2.7029\n",
            "Epoch 455/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0753 - mae: 0.7646 - val_loss: 15.2804 - val_mae: 2.7813\n",
            "Epoch 456/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9799 - mae: 0.7520 - val_loss: 15.6362 - val_mae: 2.8304\n",
            "Epoch 457/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9260 - mae: 0.6819 - val_loss: 16.3040 - val_mae: 2.8482\n",
            "Epoch 458/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0551 - mae: 0.7288 - val_loss: 13.7457 - val_mae: 2.6524\n",
            "Epoch 459/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9013 - mae: 0.6872 - val_loss: 15.3464 - val_mae: 2.7738\n",
            "Epoch 460/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0373 - mae: 0.7186 - val_loss: 13.8403 - val_mae: 2.6000\n",
            "Epoch 461/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1506 - mae: 0.7775 - val_loss: 16.3428 - val_mae: 2.8930\n",
            "Epoch 462/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0238 - mae: 0.7346 - val_loss: 17.7826 - val_mae: 2.9111\n",
            "Epoch 463/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7989 - mae: 0.6530 - val_loss: 14.0827 - val_mae: 2.6725\n",
            "Epoch 464/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9191 - mae: 0.7309 - val_loss: 16.7667 - val_mae: 2.9012\n",
            "Epoch 465/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 1.0434 - mae: 0.7781 - val_loss: 15.7330 - val_mae: 2.7645\n",
            "Epoch 466/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.8415 - mae: 0.6701 - val_loss: 14.4579 - val_mae: 2.6887\n",
            "Epoch 467/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0551 - mae: 0.7539 - val_loss: 14.9153 - val_mae: 2.7250\n",
            "Epoch 468/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0234 - mae: 0.7451 - val_loss: 14.0039 - val_mae: 2.7123\n",
            "Epoch 469/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9498 - mae: 0.7148 - val_loss: 14.3693 - val_mae: 2.6440\n",
            "Epoch 470/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9701 - mae: 0.7062 - val_loss: 15.5566 - val_mae: 2.8362\n",
            "Epoch 471/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7703 - mae: 0.6721 - val_loss: 14.6237 - val_mae: 2.6946\n",
            "Epoch 472/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.8937 - mae: 0.6952 - val_loss: 17.8221 - val_mae: 3.0388\n",
            "Epoch 473/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7233 - mae: 0.6751 - val_loss: 14.7000 - val_mae: 2.6782\n",
            "Epoch 474/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9472 - mae: 0.7096 - val_loss: 16.8410 - val_mae: 2.8801\n",
            "Epoch 475/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0020 - mae: 0.7365 - val_loss: 15.2105 - val_mae: 2.7317\n",
            "Epoch 476/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.0122 - mae: 0.6967 - val_loss: 14.6400 - val_mae: 2.6924\n",
            "Epoch 477/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.0030 - mae: 0.7173 - val_loss: 14.7586 - val_mae: 2.7521\n",
            "Epoch 478/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9642 - mae: 0.6875 - val_loss: 16.8593 - val_mae: 2.9300\n",
            "Epoch 479/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.8089 - mae: 0.6603 - val_loss: 13.6102 - val_mae: 2.5323\n",
            "Epoch 480/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.8248 - mae: 0.6565 - val_loss: 14.6814 - val_mae: 2.7306\n",
            "Epoch 481/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.8471 - mae: 0.6763 - val_loss: 16.0744 - val_mae: 2.8336\n",
            "Epoch 482/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9128 - mae: 0.6818 - val_loss: 14.9569 - val_mae: 2.7526\n",
            "Epoch 483/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.8038 - mae: 0.6457 - val_loss: 15.3938 - val_mae: 2.7386\n",
            "Epoch 484/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9491 - mae: 0.6901 - val_loss: 14.2453 - val_mae: 2.6624\n",
            "Epoch 485/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9912 - mae: 0.6850 - val_loss: 15.6483 - val_mae: 2.8982\n",
            "Epoch 486/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.0993 - mae: 0.7331 - val_loss: 13.2458 - val_mae: 2.5847\n",
            "Epoch 487/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9877 - mae: 0.7433 - val_loss: 15.3681 - val_mae: 2.7952\n",
            "Epoch 488/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7227 - mae: 0.6371 - val_loss: 13.9065 - val_mae: 2.6518\n",
            "Epoch 489/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9490 - mae: 0.7178 - val_loss: 13.4347 - val_mae: 2.6661\n",
            "Epoch 490/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.7189 - mae: 0.6130 - val_loss: 15.5396 - val_mae: 2.8165\n",
            "Epoch 491/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.7879 - mae: 0.6419 - val_loss: 16.1660 - val_mae: 2.9697\n",
            "Epoch 492/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9704 - mae: 0.7071 - val_loss: 15.2230 - val_mae: 2.7839\n",
            "Epoch 493/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9083 - mae: 0.6784 - val_loss: 13.5374 - val_mae: 2.6174\n",
            "Epoch 494/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9152 - mae: 0.6658 - val_loss: 14.5236 - val_mae: 2.7407\n",
            "Epoch 495/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.8642 - mae: 0.7020 - val_loss: 14.5979 - val_mae: 2.6956\n",
            "Epoch 496/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0426 - mae: 0.6902 - val_loss: 15.1442 - val_mae: 2.7508\n",
            "Epoch 497/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7539 - mae: 0.6511 - val_loss: 14.7553 - val_mae: 2.7262\n",
            "Epoch 498/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6586 - mae: 0.5909 - val_loss: 14.6714 - val_mae: 2.7439\n",
            "Epoch 499/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.9172 - mae: 0.6613 - val_loss: 15.3063 - val_mae: 2.8143\n",
            "Epoch 500/500\n",
            "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8119 - mae: 0.6450 - val_loss: 15.4227 - val_mae: 2.7877\n"
          ]
        }
      ],
      "source": [
        "model = build_model()  # Builds the Keras model\n",
        "history = model.fit(\n",
        "    partial_train_data,\n",
        "    partial_train_targets,\n",
        "    validation_data=(val_data, val_targets),\n",
        "    epochs=num_epochs,\n",
        "    batch_size=1,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "aFohhitbfb1-"
      },
      "outputs": [],
      "source": [
        "mae_history = history.history['val_mae']                                        # I had an issue here with the '' character, changed the 'val_mean_absolute_error’ to 'val_mae'\n",
        "all_mae_histories.append(mae_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I3Nk9kc9qe1"
      },
      "source": [
        "Training the final model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "sNwuL6nk9vUF"
      },
      "outputs": [],
      "source": [
        "model=build_model()                                                             # get a fresh compiled model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "i3q3SYel93tp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "816e35bc-38ed-4c9e-a5de-93d19e8d40d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d8a56016560>"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ],
      "source": [
        "model.fit(train_data,train_targets,epochs=80,batch_size=16,verbose=0)           # trains it on the entirety of the data (according to this plot, validation MAE stops improving significantly after 80 epochs. Past that point, you start overfitting.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "IsvzEM0yARUF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f030c01d-6cb2-453a-ebf6-c4c16121bbeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5138814.0000 - mae: 2232.3037  \n"
          ]
        }
      ],
      "source": [
        "test_mae_score, test_mae_score = model.evaluate (test_data, test_targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOuUkHE9AaDl"
      },
      "source": [
        "Final result\n",
        "\n",
        "We are still off by about $2550"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "wyOQhThCdOwb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "68628890-f1ed-4d63-dbc4-802e43f3b046"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2239.82958984375"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ],
      "source": [
        "test_mae_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Kaggle Dataset**"
      ],
      "metadata": {
        "id": "ryDMHproyH2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the CSV data from the uploaded location\n",
        "data = pd.read_csv('bank-additional-full.csv')\n",
        "\n",
        "# Display the first few rows of the dataset to ensure it's loaded correctly\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "jOAeLuOYmt9g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "063a7f28-e801-4eb4-b327-07cd67de5fa8"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  age;\"job\";\"marital\";\"education\";\"default\";\"housing\";\"loan\";\"contact\";\"month\";\"day_of_week\";\"duration\";\"campaign\";\"pdays\";\"previous\";\"poutcome\";\"emp.var.rate\";\"cons.price.idx\";\"cons.conf.idx\";\"euribor3m\";\"nr.employed\";\"y\"\n",
            "0  56;\"housemaid\";\"married\";\"basic.4y\";\"no\";\"no\";...                                                                                                                                                                          \n",
            "1  57;\"services\";\"married\";\"high.school\";\"unknown...                                                                                                                                                                          \n",
            "2  37;\"services\";\"married\";\"high.school\";\"no\";\"ye...                                                                                                                                                                          \n",
            "3  40;\"admin.\";\"married\";\"basic.6y\";\"no\";\"no\";\"no...                                                                                                                                                                          \n",
            "4  56;\"services\";\"married\";\"high.school\";\"no\";\"no...                                                                                                                                                                          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0]"
      ],
      "metadata": {
        "id": "Y2JS5XZPaRn2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "860bf562-59b7-4657-f825-fed25f9ab061"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.27224633, -0.48361547, -0.43576161, -0.25683275, -0.1652266 ,\n",
              "       -0.1764426 ,  0.81306188,  0.1166983 , -0.62624905, -0.59517003,\n",
              "        1.14850044,  0.44807713,  0.8252202 ])"
            ]
          },
          "metadata": {},
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels[0]"
      ],
      "metadata": {
        "id": "3On8En3KabDY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a7138e8e-1534-4c2b-e55f-7622c1c70b45"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = imdb.get_word_index()"
      ],
      "metadata": {
        "id": "t0kK1T7FaeSb"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in\n",
        "word_index.items()])"
      ],
      "metadata": {
        "id": "4h_uCMk4aiqk"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in\n",
        "train_data[0]])"
      ],
      "metadata": {
        "id": "L1iCPnD_atFq"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to vectorize sequences into 10,000-dimensional one-hot encoded vectors\n",
        "#def vectorize_sequences(sequences, dimension=10000):\n",
        "   # results = np.zeros((len(sequences), dimension))                             # Create an array of zeros with shape (num_sequences, 10000)\n",
        "  #  for i, sequence in enumerate(sequences):                                    # Loop over each sequence in sequences\n",
        " #       results[i, sequence] = 1                                                # Set the corresponding indices to 1 for each sequence\n",
        "  #  return results                                                              # Return the one-hot encoded matrix\n",
        "                                                                                # mean, for instance, turning the sequence [3, 5] into a 10,000-dimensional\n",
        "                                                                                # vector that would be all 0s except for indices 3 and 5, which would be 1s.\n",
        "                                                                                # Then you could use as the first layer in your network a Dense layer, capable\n",
        "                                                                                # of handling floating-point vector data"
      ],
      "metadata": {
        "id": "wQcnb_kvatg1"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to vectorize sequences into 10,000-dimensional one-hot encoded vectors\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))                             # Create an array of zeros with shape (num_sequences, 10000)\n",
        "    for i, sequence in enumerate(sequences):                                    # Loop over each sequence in sequences\n",
        "        for j in sequence:                                                       # Loop over each index in the sequence\n",
        "            if j >= 0 and j < dimension:                                        # Ensure index is within bounds\n",
        "                results[i, int(j)] = 1                                          # Set the corresponding index to 1 for each sequence (convert to int)\n",
        "    return results                                                              # Return the one-hot encoded matrix\n",
        "                                                                                # mean, for instance, turning the sequence [3, 5] into a 10,000-dimensional\n",
        "                                                                                # vector that would be all 0s except for indices 3 and 5, which would be 1s.\n",
        "                                                                                # Then you could use as the first layer in your network a Dense layer, capable\n",
        "                                                                                # of handling floating-point vector data                                    # Convert the test_data into one-hot encoded vectors\n",
        "\n",
        "# Convert labels to float32 data type\n",
        "y_train = np.asarray(train_labels).astype('float32')                            # Convert the train_labels to float32\n",
        "y_test = np.asarray(test_labels).astype('float32')                              # Convert the test_labels to float32"
      ],
      "metadata": {
        "id": "sfMmA6IZa2Ig"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the 'models' object from Keras\n",
        "from tensorflow.keras import models\n",
        "# Import the 'layers' module from Keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Re-build and re-train the model for 4 epochs based on validation performance\n",
        "model = models.Sequential()                                                     # Initialize a new sequential model\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))            # Add first Dense layer with 16 units, relu activation, and input shape of 10,000\n",
        "model.add(layers.Dense(16, activation='relu'))                                  # Add second Dense layer with 16 units and relu activation\n",
        "model.add(layers.Dense(1, activation='sigmoid'))                                # Add output layer with 1 unit and sigmoid activation for binary classification"
      ],
      "metadata": {
        "id": "oXwa3RaMa98k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7814af9d-280f-4c55-de04-7f1bae9572cc"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy',\n",
        "metrics=['accuracy'])\n",
        "                                                                                # it’s best to use the binary_crossentropy loss. It isn’t the only viable choice:\n",
        "                                                                                # you could use, for instance, mean_squared_error. But crossentropy is\n",
        "                                                                                # usually the best choice when you’re dealing with models that output\n",
        "                                                                                # probabilities.ompile(optimizer='rmsprop', loss='binary_crossentropy',"
      ],
      "metadata": {
        "id": "cXK2iKe3bFr_"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "metadata": {
        "id": "_CcfsjXKb1qJ"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "# Train the model for 20 epochs with mini-batches of 512 samples and validation data\n",
        "history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))\n",
        "# Train the model on partial_x_train and partial_y_train, validate on x_val and y_val"
      ],
      "metadata": {
        "id": "SZMNh36Fb2uW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "081e40cf-fb4c-4fd0-8960-5f4a249d6de0"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - acc: 0.6845 - loss: 0.6172 - val_acc: 0.8558 - val_loss: 0.4330\n",
            "Epoch 2/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - acc: 0.8852 - loss: 0.3741 - val_acc: 0.8686 - val_loss: 0.3426\n",
            "Epoch 3/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - acc: 0.9128 - loss: 0.2706 - val_acc: 0.8850 - val_loss: 0.2943\n",
            "Epoch 4/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - acc: 0.9294 - loss: 0.2175 - val_acc: 0.8860 - val_loss: 0.2785\n",
            "Epoch 5/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - acc: 0.9404 - loss: 0.1816 - val_acc: 0.8825 - val_loss: 0.2890\n",
            "Epoch 6/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - acc: 0.9539 - loss: 0.1527 - val_acc: 0.8785 - val_loss: 0.3034\n",
            "Epoch 7/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - acc: 0.9607 - loss: 0.1302 - val_acc: 0.8811 - val_loss: 0.3007\n",
            "Epoch 8/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - acc: 0.9675 - loss: 0.1091 - val_acc: 0.8846 - val_loss: 0.3023\n",
            "Epoch 9/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - acc: 0.9735 - loss: 0.0940 - val_acc: 0.8759 - val_loss: 0.3255\n",
            "Epoch 10/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - acc: 0.9769 - loss: 0.0826 - val_acc: 0.8818 - val_loss: 0.3314\n",
            "Epoch 11/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - acc: 0.9846 - loss: 0.0659 - val_acc: 0.8801 - val_loss: 0.3469\n",
            "Epoch 12/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - acc: 0.9861 - loss: 0.0596 - val_acc: 0.8777 - val_loss: 0.3819\n",
            "Epoch 13/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - acc: 0.9896 - loss: 0.0499 - val_acc: 0.8757 - val_loss: 0.3863\n",
            "Epoch 14/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - acc: 0.9910 - loss: 0.0427 - val_acc: 0.8710 - val_loss: 0.4154\n",
            "Epoch 15/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - acc: 0.9939 - loss: 0.0362 - val_acc: 0.8735 - val_loss: 0.4479\n",
            "Epoch 16/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - acc: 0.9952 - loss: 0.0304 - val_acc: 0.8734 - val_loss: 0.4597\n",
            "Epoch 17/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - acc: 0.9960 - loss: 0.0251 - val_acc: 0.8743 - val_loss: 0.4785\n",
            "Epoch 18/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - acc: 0.9971 - loss: 0.0223 - val_acc: 0.8677 - val_loss: 0.5088\n",
            "Epoch 19/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - acc: 0.9986 - loss: 0.0166 - val_acc: 0.8721 - val_loss: 0.5197\n",
            "Epoch 20/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - acc: 0.9991 - loss: 0.0135 - val_acc: 0.8716 - val_loss: 0.5430\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "metadata": {
        "id": "LHKzGAEncHZu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "56c19b1d-4b87-4958-b49f-fe8061e0e235"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['acc', 'loss', 'val_acc', 'val_loss'])"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-build and re-train the model for 4 epochs based on validation performance\n",
        "model = models.Sequential()  # Initialize a new sequential model\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))  # Add first Dense layer with 16 units, relu activation, and input shape of 10,000\n",
        "model.add(layers.Dense(16, activation='relu'))  # Add second Dense layer with 16 units and relu activation\n",
        "model.add(layers.Dense(1, activation='sigmoid'))  # Add output layer with 1 unit and sigmoid activation for binary classification\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])  # Compile the new model\n",
        "model.fit(x_train, y_train, epochs=4, batch_size=512)  # Train the new model on the entire x_train and y_train for 4 epochs"
      ],
      "metadata": {
        "id": "Tdyev-lJcMAi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2d66fdb1-02f0-4f62-95a3-43beb69fd156"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.7501 - loss: 0.5730\n",
            "Epoch 2/4\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9010 - loss: 0.3037\n",
            "Epoch 3/4\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9194 - loss: 0.2284\n",
            "Epoch 4/4\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9342 - loss: 0.1902\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d8a4e1cf610>"
            ]
          },
          "metadata": {},
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(x_test, y_test)  # Evaluate the model on x_test and y_test"
      ],
      "metadata": {
        "id": "n1ZajsCncQGK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "20c44c2d-f074-4f86-eab7-a01c7cda6249"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8839 - loss: 0.2852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G34Nx0iv7bhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FaWo_-8x7bfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mxgz2K807bcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "scYm1ZaD7baI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2IZJsidm7bXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qTeo27h47bVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wSRbqL147bS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PNjPZbpx7bQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rVgJtU5X7bN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m62JBNjj7bLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JxlwqQRD7bI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wdeShxqT7bAc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}